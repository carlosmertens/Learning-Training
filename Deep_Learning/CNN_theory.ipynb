{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:** Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting.\n",
    "\n",
    "We have seen that we can create a neural network model with a **Multilayer Perceptron** and we have tested it on a handwritten images dataset (Mnist). The problem is that we needed to convert the image in a vector in order to feed it to the network and MLP uses a fully connected layers creating a great deal of parameters. That means a great deal of computation and losing a lot of details on the images. \n",
    "\n",
    "The better option for image classification is Convolutional Neural Network or CNN.\n",
    "\n",
    "* It works with matrix, hence we can feed the model converting the image in a matrix.\n",
    "* Uses sparsely connected layers, saving a lot of time in computation.\n",
    "\n",
    "\n",
    "### Local Connectivity\n",
    "\n",
    "Locally connected layers uses far fewer parameters than a densely connected layer. The idea is that we convert the image in a matrix and divide it in 4 different areas. Every area will be connected to its specific node and the node will only learn about that area in the matrix (image). Then the nodes report to the output layer where the information learned is combined. The hidden nodes work also together with a **sharing weights system**. We can expand the number of nodes creating a **collection of hidden layers** where every collection will be responsible for learning a specific area of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Using the idea of the locally connected layer, we present **the convolutional layer**. We select the width and height of the convolutional window which are the weights represented on a grid called a filter, and then the window is slided vertically and horizontally on the matrix. At each position, the window defines a small section of pixels in the matrix and connects it to a specific single hidden node called the convolutional layer. \n",
    "\n",
    "If we want to find more patterns in the image, we can use more filter to learn better the pixels in the image. Every filter will be a collection of nodes with different weights and bias. This collection of nodes are called **feature maps** or **activation maps**.\n",
    "\n",
    "Gray scale images are interpreted as 2D array with height and width color images are interpreted as 3D array with height, width and depth (In RGB images, the depth value is 3). It is considered as a stack of 3 two-dimensional matrices. Hence, the filter or the convolutional window needs to be also a 3 dimensional grid.\n",
    "\n",
    "### Create Convolutional Layers\n",
    "\n",
    "```\n",
    "# Import \n",
    "from keras.layers import Conv2d\n",
    "\n",
    "# Create architecture\n",
    "Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "Must arguments:\n",
    "\n",
    "- ***filters*** - The number of filters.\n",
    "- ***kernel_size*** - Number specifying both the height and width of the (square) convolution window.\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "- ***strides*** - The stride of the convolution. If you don't specify anything, strides is set to 1.\n",
    "- ***padding*** - One of 'valid' or 'same'. If you don't specify anything, padding is set to 'valid'.\n",
    "- ***activation*** - Typically 'relu'. If you don't specify anything, no activation is applied. You are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks.\n",
    "\n",
    "**NOTE**: It is possible to represent both kernel_size and strides as either a number or a tuple. \n",
    "\n",
    "When using your convolutional layer as the first layer (appearing after the input layer) in a model, you must provide an additional ***input_shape*** argument:\n",
    "\n",
    "- ***input_shape*** - Tuple specifying the height, width, and depth (in that order) of the input.\n",
    "\n",
    "Do not include the input_shape argument if the convolutional layer is not the first layer in your network.\n",
    "\n",
    "### Stride and Padding\n",
    "\n",
    "We can control the behaviour of the convolutional layers by specifying the number of filters and the size of filters:\n",
    "* To increase the number of nodes, increase the numbers of filters\n",
    "* To increase the size of the detected patterns, increase the size of the filter\n",
    "\n",
    "We also have the **hyperparameters of the stride** of the convolutional which is the amount by which the filter slides over the image. \n",
    "A stride value of 1 makes the convolutional layer the same width and height as the input image. But if we set the value to 2, when we do not have too many details in the image, then the convolutional layer will differ from the input images and the filter will fall outside the image input. We then need to also set **padding** to create another dummy column in the image input to have the same size of the convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Say I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1). Then, say I'd like the next layer to be a convolutional layer with 16 filters, each with a width and height of 2. When performing the convolution, I'd like the filter to jump two pixels at a time. I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros. Then, to construct this convolutional layer, I would use the following line of code:\n",
    "```\n",
    "Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))\n",
    "```\n",
    "\n",
    "### Example 2\n",
    "\n",
    "Say I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 filters, each with a height and width of 3. When performing the convolution, I'd like the filter to jump 1 pixel at a time. I want the convolutional layer to see all regions of the previous layer, and so I don't mind if the filter hangs over the edge of the previous layer when it's performing the convolution. Then, to construct this convolutional layer, I would use the following line of code:\n",
    "```\n",
    "Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')\n",
    "```\n",
    "\n",
    "### Example 3\n",
    "\n",
    "If you look up code online, it is also common to see convolutional layers in Keras in this format:\n",
    "```\n",
    "Conv2D(64, (2,2), activation='relu')\n",
    "```\n",
    "In this case, there are 64 filters, each with a size of 2x2, and the layer has a ReLU activation function. The other arguments in the layer use the default values, so the convolution uses a stride of 1, and the padding has been set to 'valid'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimencionality of the CNN\n",
    "\n",
    "Create a CNN and change the parameters. Observe the architecture\n",
    "\n",
    "### Formula: Number of Parameters in a Convolutional Layer\n",
    "\n",
    "The number of parameters in a convolutional layer depends on the supplied values of ***filters***, ***kernel_size***, and ***input_shape***.\n",
    "\n",
    "The number of parameters in the convolutional layer is given by:\n",
    "```\n",
    "filters * kernel_size(height * width) * input_shape + Bias(one bias per filter).\n",
    "```\n",
    "\n",
    "### Shape of a Convolutional Layer\n",
    "\n",
    "The shape of a convolutional layer depends on the supplied values of ***kernel_size***, ***input_shape***, ***padding***, and ***stride***.\n",
    "\n",
    "The ***depth*** of the convolutional layer will always equal the number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      80        \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, strides=2, padding='valid',\n",
    "                 activation='relu', input_shape=(200, 200, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                 activation='relu', input_shape=(128, 128, 3)))\n",
    "\n",
    "model.summary()\n",
    "# The number of parameters is (32 x 3 x 3 x 3) + 32 = 896\n",
    "# The depth of a convolutional layer is 32\n",
    "# The width of the convolutional layer is 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "***Remember:*** Convolutional layers are stack of features maps where we have one feature map for each filter. A complicated dataset will require large number of filters responsible for finding a pattern which means a high number of parameters.\n",
    "\n",
    "\n",
    "Pooling Layers are often taking convolutional layers as input. It is a method to reduce the dimencionality of the CNN. Higher dimencionality means more parameters which can lead to overfitting. Therefore, the goal of the pooling layer is to avoid overfitting for cases where our model needs a lot of features (filters)\n",
    "\n",
    "There are 2 types:\n",
    "- **Max Pooling Layer** - It is defined by a window size and stride as the convolutional layer. I will go stride by stride and will select the max value in each stride to create a new feature map that has been reduced in width and height. \n",
    "- **Global Average Pooling Layer** - This is a more extreme type of dimensionality reduction. It takes a stack of feuture maps and computes the average value of the nodes for each map in the stack. Therefore, it takes a 3D array and turns it into a vector.\n",
    "\n",
    "### Create Max Pooling Layers\n",
    "\n",
    "```\n",
    "# Import\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# Create architecture\n",
    "MaxPooling2D(pool_size, strides, padding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "Must arguments:\n",
    "\n",
    "- ***pool_size*** - Number specifying the height and width of the pooling window.\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "- ***strides*** - The vertical and horizontal stride. If you don't specify anything, strides will default to *pool_size*.\n",
    "- ***padding*** - One of 'valid' or 'same'. If you don't specify anything, padding is set to 'valid'.\n",
    "\n",
    "**NOTE**: It is possible to represent both *pool_size* and *strides* as either a number or a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Say I'm constructing a CNN, and I'd like to reduce the dimensionality of a convolutional layer by following it with a max pooling layer. Say the convolutional layer has size (100, 100, 15), and I'd like the max pooling layer to have size (50, 50, 15). I can do this by using a 2x2 window in my max pooling layer, with a stride of 2, which could be constructed in the following line of code:\n",
    "\n",
    "```\n",
    "MaxPooling2D(pool_size=2, strides=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 15)        0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, input_shape=(100, 100, 15)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs for Image Classification\n",
    "\n",
    "- The most used layers in CNNs are Fully Connected Layers, Flatten Layers, Convolutional Layers and Pooling Layers and they need to be arrange carefully to design a CNN architecture.\n",
    "\n",
    "- The model must accept an image array as input. It is common to resize the images to a square with the spatial dimensions equal to the power of two. For example: height*width*depth = 32 * 32(pixels) * 1(or 3, if it is a color image) since images are interpreted by computers as 3D array.\n",
    "\n",
    "- As the image passes through the model, the convolutional layers make the array deeper while the pooling layer decrease the spatial dimensions. \n",
    "\n",
    "- It is common practice to set the prameters *strides*=1 and *padding*='same' so the with and height is the same as the previous layer.\n",
    "\n",
    "- Ususally the numbers of filters increase in sequence like 16, 32, 64, and so on.\n",
    "\n",
    "- Usaually the pooling layers are set to *pool_size*=2 and *stride*=2 so they are half of the previous layer. \n",
    "\n",
    "- Usually, when the initial array has been transformed into a reduced dimension and deep array, it is feed to a fully connected layer to process the information. In same cases, we will flat the array al the way to converted to a vector and then feed it to the fully connected layer.\n",
    "\n",
    "- Finally, it is common practice to connect the dense layer to a final dense layer with activation *softmax* for classification according to the objects classes we have. Note that, it is common practice to use *relu* activation for all other layers.\n",
    "\n",
    "**Things to Remember**\n",
    "\n",
    "- Always add a ReLU activation function to the Conv2D layers in your CNN. With the exception of the final layer in the network, Dense layers should also have a ReLU activation function.\n",
    "- When constructing a network for classification, the final layer in the network should be a Dense layer with a softmax activation function. The number of nodes in the final layer should equal the total number of classes in the dataset.\n",
    "Have fun! If you start to feel discouraged, we recommend that you check out Andrej Karpathy's tumblr with user-submitted loss functions, corresponding to models that gave their owners some trouble. Recall that the loss is supposed to decrease during training. These plots show very different behavior :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               512500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 528,054\n",
      "Trainable params: 528,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# The network begins with a sequence of three convolutional layers, followed\n",
    "# by max pooling layers. These first six layers are designed to take the input\n",
    "# array of image pixels and convert it to an array where all of the spatial\n",
    "# information has been squeezed out, and only information encoding the content\n",
    "# of the image remains. The array is then flattened to a vector in the seventh\n",
    "# layer of the CNN. It is followed by two dense layers designed to further\n",
    "# elucidate the content of the image. The final layer has one entry for each\n",
    "# object class in the dataset, and has a softmax activation function, so that it\n",
    "# returns probabilities.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Display model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation \n",
    "\n",
    "https://keras.io/layers/convolutional/\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "* Udacity, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
