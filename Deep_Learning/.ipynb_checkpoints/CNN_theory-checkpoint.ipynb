{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:** Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we can create a neural network model with a **Multilayer Perceptron** and we have tested it on a handwritten images dataset (Mnist). The problem is that we needed to convert the image in a vector in order to feed it to the network and MLP uses a fully connected layers creating a great deal of parameters. That means a great deal of computation and losing a lot of details on the images. \n",
    "\n",
    "The better option for image classification is Convolutional Neural Network or CNN.\n",
    "\n",
    "* It works with matrix, hence we can feed the model converting the image in a matrix.\n",
    "* Uses sparsely connected layers, saving a lot of time in computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Connectivity**\n",
    "\n",
    "Locally connected layers uses far fewer parameters than a densely connected layer. The idea is that we convert the image in a matrix and divide it in 4 different areas. Every area will be connected to its specific node and the node will only learn about that area in the matrix (image). Then the nodes report to the output layer where the information learned is combined. The hidden nodes work also together with a **sharing weights system**. We can expand the number of nodes creating a **collection of hidden layers** where every collection will be responsible for learning a specific area of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers**\n",
    "\n",
    "Using the idea of the locally connected layer, we present **the convolutional layer**. We select the width and height of the convolutional window which are the weights represented on a grid called a filter, and then the window is slided vertically and horizontally on the matrix. At each position, the window defines a small section of pixels in the matrix and connects it to a specific single hidden node called the convolutional layer. \n",
    "\n",
    "If we want to find more patterns in the image, we can use more filter to learn better the pixels in the image. Every filter will be a collection of nodes with different weights and bias. This collection of nodes are called **feature maps** or **activation maps**.\n",
    "\n",
    "Gray scale images are interpreted as 2D array with height and width color images are interpreted as 3D array with height, width and depth (In RGB images, the depth value is 3). It is considered as a stack of 3 two-dimensional matrices. Hence, the filter or the convolutional window needs to be also a 3 dimensional grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stride and Padding**\n",
    "\n",
    "We can control the behaviour of the convolutional layers by specifying the number of filters and the size of filters:\n",
    "* To increase the number of nodes, increase the numbers of filters\n",
    "* To increase the size of the detected patterns, increase the size of the filter\n",
    "\n",
    "We also have the **hyperparameters of the stride** of the convolutional which is the amount by which the filter slides over the image. \n",
    "A stride value of 1 makes the convolutional layer the same width and height as the input image. But if we set the value to 2, when we do not have too many details in the image, then the convolutional layer will differ from the input images and the filter will fall outside the image input. We then need to also set **padding** to create another dummy column in the image input to have the same size of the convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the module:\n",
    "\n",
    "```\n",
    "from keras.layers import Conv2d\n",
    "```\n",
    "\n",
    "**Create** a convolutional layer by using the following format:\n",
    "\n",
    "```\n",
    "Conv2D(filters, kernel_size, strides, padding, activation='relu', input_shape)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arguments**\n",
    "\n",
    "Must arguments:\n",
    "\n",
    "- ***filters*** - The number of filters.\n",
    "- ***kernel_size*** - Number specifying both the height and width of the (square) convolution window.\n",
    "\n",
    "Optional arguments:\n",
    "\n",
    "- ***strides*** - The stride of the convolution. If you don't specify anything, strides is set to 1.\n",
    "- ***padding*** - One of 'valid' or 'same'. If you don't specify anything, padding is set to 'valid'.\n",
    "- ***activation*** - Typically 'relu'. If you don't specify anything, no activation is applied. You are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks.\n",
    "\n",
    "**NOTE**: It is possible to represent both kernel_size and strides as either a number or a tuple. \n",
    "\n",
    "When using your convolutional layer as the first layer (appearing after the input layer) in a model, you must provide an additional ***input_shape*** argument:\n",
    "\n",
    "- ***input_shape*** - Tuple specifying the height, width, and depth (in that order) of the input.\n",
    "\n",
    "Do not include the input_shape argument if the convolutional layer is not the first layer in your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation** can be find here:\n",
    "\n",
    "https://keras.io/layers/convolutional/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "\n",
    "Say I'm constructing a CNN, and my input layer accepts grayscale images that are 200 by 200 pixels (corresponding to a 3D array with height 200, width 200, and depth 1). Then, say I'd like the next layer to be a convolutional layer with 16 filters, each with a width and height of 2. When performing the convolution, I'd like the filter to jump two pixels at a time. I also don't want the filter to extend outside of the image boundaries; in other words, I don't want to pad the image with zeros. Then, to construct this convolutional layer, I would use the following line of code:\n",
    "\n",
    "```\n",
    "Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "\n",
    "Say I'd like the next layer in my CNN to be a convolutional layer that takes the layer constructed in Example 1 as input. Say I'd like my new layer to have 32 filters, each with a height and width of 3. When performing the convolution, I'd like the filter to jump 1 pixel at a time. I want the convolutional layer to see all regions of the previous layer, and so I don't mind if the filter hangs over the edge of the previous layer when it's performing the convolution. Then, to construct this convolutional layer, I would use the following line of code:\n",
    "\n",
    "```\n",
    "Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**\n",
    "\n",
    "If you look up code online, it is also common to see convolutional layers in Keras in this format:\n",
    "\n",
    "```\n",
    "Conv2D(64, (2,2), activation='relu')\n",
    "```\n",
    "\n",
    "In this case, there are 64 filters, each with a size of 2x2, and the layer has a ReLU activation function. The other arguments in the layer use the default values, so the convolution uses a stride of 1, and the padding has been set to 'valid'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Acknowledgement***\n",
    "\n",
    "* Udacity, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
